# Fall 2022: CS 6375 Machine Learning (Graduate Version)

## Course Videos
Video Lectures: https://www.youtube.com/playlist?list=PLGod0_zT9w93y14TbXOVQpmvrnLssdPkp

These are video recordings uploaded to youtube from my previous graduate version of this course. The course content will be very similar so I would strongly encourage all of you to go through the videos both before and after each course.

## Course Evaluation
- 4 Assignments (50%)
- Mid Term Examination (25%)
- Final Project (25%)

## Office Hours
- Monday 3 pm -  3:50 pm  (my office: ECSS 3.405) 
- Wednesday 3 pm - 3:50 pm (by appointment)

## Course Lecture Slides
I will be placing the annotated course slides on this github repo

## Demos: Google Colab Links and Other Links
- Linear Regression (Housing Demo): https://colab.research.google.com/drive/1myH2V4xbKXZzdF-49ma_vWHLfArTyO2W
- Polynomial Regression (Overfitting & Underfitting): https://colab.research.google.com/drive/1ynzXK32eLqpPdh6l7IOqW7ajnCkXKhwH
- Perceptrons: https://colab.research.google.com/drive/1eNG_bEj_gfq1yzjBaJlRA8D-Bdt-zhjR
- SVMs: https://colab.research.google.com/drive/1z368aCHvKDy92E0dJXtQnTFgF3O1HOWo
- Interactive SVM Demo: https://jgreitemann.github.io/svm-demo
- Decision Trees: https://colab.research.google.com/drive/1RKC12-hsxRx7GqogkQjN0ylqFy6ay2As#scrollTo=8c5bbca1
- Nearest Neighbor: https://colab.research.google.com/drive/1coQx_fGMyiOhli6xjETUaXfy3AX1PqWX#scrollTo=8fb86b64

## Syllabus
- Introduction and About ML
- Regression (Linear and Polynomial)
- Perceptron
- Support Vector Machines
- Nearest Neighbor Methods
- Decision Trees
- Naive Bayes
- Logistic Regression
- Unsupervised Learning: Clustering
- Learning Theory and VC Dimensions
- Bias and Variance Tradeoff
- Ensembles, Boosting and Bagging
- Gaussian Mixture Models
- Neural Networks
- Evaluation of ML Algorithms and Practical Aspects

## Textbooks
- Pattern Recognition and Machine Learning, Christopher Bishop 2006
https://www.amazon.com/dp/0387310738?
https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf
- The Elements of Statistical Learning, Trevor Hastie and Robert Tibshirani
https://web.stanford.edu/~hastie/ElemStatLearn/
https://www.amazon.com/dp/0387848576?
- Machine Learning, Tom Mitchell
https://www.amazon.com/dp/0070428077?
- Bayesian Reasoning and Machine Learning by David Barber (http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)
- Boyd for Convex Optimization parts (e.g., Duality, Gradient Descent, etc.): https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf
- Andrew Ng's Lecture Notes: http://cs229.stanford.edu/notes2020spring/cs229-notes1.pdf

## Additional Reading Material
- Introduction and Regression: Bishop, Ch. 1 and Sections 1 & 2 of Andrew Ng's Lecture notes
- Perceptron: Bishop, Ch. 7 for a different perspective, Barber, Ch. 17.5
- SVMs, Duality: Boyd, Ch 5, https://web.archive.org/web/20210126032916/http://www.engr.mun.ca/~baxter/Publications/LagrangeForSVMs.pdf
- SVMs and Slack: Bishop Ch 7.1,
- Nearest Neighbor Methods: Bishop Ch 2.5.2, https://towardsdatascience.com/a-simple-introduction-to-k-nearest-neighbors-algorithm-b3519ed98e
- Decision trees: Bishop, Ch 14.4, Mitchel Ch 3 (https://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/mitchell-dectrees.pdf), Nicholas Rouzzi's notes (https://personal.utdallas.edu/~nrr150130/gmbook/prob_basics.html), (https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation)
- Bayesian Methods: Bishop, Ch 2.1 - 2.4, Ch 4.2
- Naive Bayes: Ch 4.2.3, 8.2.2, http://www.cs.cmu.edu/~guestrin/Class/10701-S06/Slides/naivebayes-logisticregression.pdf
- Logistic Regression: Ch. 4.3, 4.5, Sec. 5 in Andrew Ng's notes, https://joparga3.github.io/standford_logistic_regression/
- Clustering: Bishop: Ch 9.1, https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1
- VC Dimension and Learning Theory: https://www.cs.princeton.edu/~rlivni/cos511/lectures/lect1.pdf, cs.utexas.edu/~klivans/f06lec2.pdf, https://www.cse.wustl.edu/~bjuba/cse513t/f16/notes/l10.pdf, https://towardsdatascience.com/measuring-the-power-of-a-classifier-c765a7446c1c, https://www.cs.cmu.edu/~epxing/Class/10701/slides/lecture16-VC.pdf
- Bias/Variance Tradeoff, Boosting and Bagging: Hastie et al: Ch. 14.3.6, 14.3.8, 14.3.9, 14.3.12 and Hastie et al Ch. 8.7 & Ch. 15. Also https://cseweb.ucsd.edu//~yfreund/papers/IntroToBoosting.pdf.
- EM Algorithm and Gaussian Mixture Models: https://f.hubspotusercontent40.net/hubfs/8111846/Unicon_October2020/pdf/bilmes-em-algorithm.pdf
